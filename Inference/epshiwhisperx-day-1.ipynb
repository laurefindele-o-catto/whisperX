{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":129276,"databundleVersionId":15506988,"sourceType":"competition"}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"We will use laurefindele-o-catto/whisperX \n\nwhisper model - tsugitsugi converted\nalignment model - sohan.wav2vec2 - from dl sprint 22 ","metadata":{}},{"cell_type":"code","source":"# Clone YOUR modified WhisperX repo with Bengali alignment support\n!git clone https://github.com/laurefindele-o-catto/whisperX\n%cd whisperX\n\n\n# Install your modified WhisperX\n!pip install -e .\n\n# Install other dependencies\n!pip install transformers torch torchaudio\n!pip install huggingface_hub pandas numpy librosa soundfile\n!pip install accelerate evaluate jiwer tqdm\n\nprint(\"All dependencies installed!\")\nprint(\"Using YOUR modified WhisperX with Bengali wav2vec2 alignment\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**RESTART HERE!**","metadata":{}},{"cell_type":"markdown","source":"**Optimzed Environment**","metadata":{}},{"cell_type":"code","source":"# Handle cuDNN loading issues for GPU optimization\nimport os\n\noriginal_ld_path = os.environ.get(\"LD_LIBRARY_PATH\", \"\")\ncudnn_paths = [\n    \"/opt/conda/lib/python3.10/site-packages/nvidia/cudnn/lib/\",\n    \"/opt/conda/lib/python3.11/site-packages/nvidia/cudnn/lib/\",\n    \"/usr/local/cuda/lib64/\"\n]\n\nfor cudnn_path in cudnn_paths:\n    if os.path.exists(cudnn_path):\n        os.environ['LD_LIBRARY_PATH'] = original_ld_path + \":\" + cudnn_path\n        print(f\"cuDNN path added: {cudnn_path}\")\n        break\n\n# Suppress warnings for cleaner output\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nprint(\"Environment optimized!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:26:06.443943Z","iopub.execute_input":"2026-02-10T18:26:06.444271Z","iopub.status.idle":"2026-02-10T18:26:06.452949Z","shell.execute_reply.started":"2026-02-10T18:26:06.444241Z","shell.execute_reply":"2026-02-10T18:26:06.452217Z"}},"outputs":[{"name":"stdout","text":"cuDNN path added: /usr/local/cuda/lib64/\nEnvironment optimized!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"**Import and Setup**","metadata":{}},{"cell_type":"code","source":"import whisperx\nimport torch\nimport torchaudio\nimport numpy as np\nimport pandas as pd\nimport os\nimport gc\nimport glob\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Check GPU availability\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n    print(f\"CUDA Version: {torch.version.cuda}\")\nelse:\n    print(\"Running on CPU - will be slower but functional\")\n\nprint(f\"PyTorch Version: {torch.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:26:11.684233Z","iopub.execute_input":"2026-02-10T18:26:11.684522Z","iopub.status.idle":"2026-02-10T18:26:16.370577Z","shell.execute_reply.started":"2026-02-10T18:26:11.684495Z","shell.execute_reply":"2026-02-10T18:26:16.369917Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU: Tesla P100-PCIE-16GB\nGPU Memory: 17.1 GB\nCUDA Version: 12.6\nPyTorch Version: 2.8.0+cu126\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"**Configs and Paths**","metadata":{}},{"cell_type":"code","source":"# Your Custom Bengali Whisper Model (CTranslate2 format on HuggingFace)\nBENGALI_WHISPER_MODEL = \"pawmeow/whisper-tugstugi-bengali-ct2\"\n\n\n# Data Paths - UPDATE THESE WITH ACTUAL PATHS\nTRAIN_AUDIO_PATH = \"/kaggle/input/dl-sprint-4-0-bengali-long-form-speech-recognition/transcription/transcription/train/audio\"\nTRAIN_ANNOTATION_PATH = \"/kaggle/input/dl-sprint-4-0-bengali-long-form-speech-recognition/transcription/transcription/train/annotation\"\nTEST_AUDIO_PATH = \"/kaggle/input/dl-sprint-4-0-bengali-long-form-speech-recognition/transcription/transcription/test/audio\"\n\n# Output Configuration\nOUTPUT_CSV_PATH = \"submission.csv\"\n\n# Processing Configuration\nCONFIG = {\n    'batch_size': 1 if device == \"cuda\" else 1,\n    'compute_type': \"float32\" if device == \"cuda\" else \"int8\",\n    'chunk_size': 30,\n    'use_alignment': True, \n    #'beam_size': 5, default 5, higher increases accuracy\n    'language': 'bn',\n    'max_audio_length': None,\n    'vad_onset': 0.500,\n    'vad_offset': 0.363,\n}\n\nprint(\"Configuration:\")\nprint(f\"Whisper Model: {BENGALI_WHISPER_MODEL}\")\nprint(f\"Alignment: Bengali wav2vec2 (from alignment.py)\")\nfor key, value in CONFIG.items():\n    print(f\"  {key}: {value}\")\n\n# Verify paths exist\npaths_to_check = {\n    \"Train Audio\": TRAIN_AUDIO_PATH,\n    \"Train Annotations\": TRAIN_ANNOTATION_PATH,\n    \"Test Audio\": TEST_AUDIO_PATH\n}\n\nprint(\"\\nPath Verification:\")\nfor name, path in paths_to_check.items():\n    if os.path.exists(path):\n        count = len([f for f in os.listdir(path) if f.endswith('.wav' if 'audio' in name.lower() else '.txt')])\n        print(f\"{name}: {path} ({count} files)\")\n    else:\n        print(f\"{name}: {path} (UPDATE THIS PATH!)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:27:57.534294Z","iopub.execute_input":"2026-02-10T18:27:57.534919Z","iopub.status.idle":"2026-02-10T18:27:57.552104Z","shell.execute_reply.started":"2026-02-10T18:27:57.534889Z","shell.execute_reply":"2026-02-10T18:27:57.551426Z"}},"outputs":[{"name":"stdout","text":"Configuration:\nWhisper Model: pawmeow/whisper-tugstugi-bengali-ct2\nAlignment: Bengali wav2vec2 (from alignment.py)\n  batch_size: 1\n  compute_type: float32\n  chunk_size: 30\n  use_alignment: True\n  language: bn\n  max_audio_length: None\n  vad_onset: 0.5\n  vad_offset: 0.363\n\nPath Verification:\nTrain Audio: /kaggle/input/dl-sprint-4-0-bengali-long-form-speech-recognition/transcription/transcription/train/audio (113 files)\nTrain Annotations: /kaggle/input/dl-sprint-4-0-bengali-long-form-speech-recognition/transcription/transcription/train/annotation (113 files)\nTest Audio: /kaggle/input/dl-sprint-4-0-bengali-long-form-speech-recognition/transcription/transcription/test/audio (24 files)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"**Model**","metadata":{}},{"cell_type":"code","source":"print(\"ü§ñ Loading the Custom Bengali Whisper Model...\")\n\n\nfrom omegaconf import ListConfig\nfrom omegaconf.base import ContainerMetadata, Metadata\nfrom omegaconf.nodes import AnyNode\nfrom typing import Any, List, Dict, Optional\nfrom collections import defaultdict\nimport torch\nfrom torch.torch_version import TorchVersion\nfrom pyannote.audio.core.model import Introspection\nfrom pyannote.audio.core.task import Specifications, Problem, Resolution\n\ntorch.serialization.add_safe_globals([\n    ListConfig,ContainerMetadata,Metadata,\n    AnyNode,    Any,\n    List,    Dict,\n    Optional,    list,   \n    dict,  tuple, set,\n    defaultdict,   # collections.defaultdict\n    TorchVersion, Introspection, Specifications, Problem, Resolution\n])\n\n\nwhisper_model = None\ntry:\n    whisper_model = whisperx.load_model(\n        BENGALI_WHISPER_MODEL,\n        device=device,\n        compute_type=CONFIG['compute_type'],\n        language=CONFIG['language']\n    )\n    print(f\"Custom Bengali Whisper model loaded: {BENGALI_WHISPER_MODEL}\")\nexcept Exception as e:\n    print(f\"‚ùå Error loading custom model: {e}\")\n    # whisper_model = whisperx.load_model(\n    #     \"large-v2\",\n    #     device=device,\n    #     compute_type=CONFIG['compute_type'],\n    #     language=CONFIG['language']\n    # )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:42:35.422688Z","iopub.execute_input":"2026-02-10T18:42:35.423466Z","iopub.status.idle":"2026-02-10T18:42:37.444098Z","shell.execute_reply.started":"2026-02-10T18:42:35.423429Z","shell.execute_reply":"2026-02-10T18:42:37.443451Z"}},"outputs":[{"name":"stdout","text":"ü§ñ Loading the Custom Bengali Whisper Model...\n2026-02-10 18:42:37 - whisperx.vads.pyannote - INFO - Performing voice activity detection using Pyannote...\n","output_type":"stream"},{"name":"stderr","text":"INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint whisperX/whisperx/assets/pytorch_model.bin`\n","output_type":"stream"},{"name":"stdout","text":"Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\nModel was trained with torch 1.10.0+cu102, yours is 2.8.0+cu126. Bad things might happen unless you revert torch to 1.x.\n‚ùå Error loading custom model: Inference.__init__() got an unexpected keyword argument 'token'\nFalling back to standard large-v2...\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"**Alignment Model**","metadata":{}},{"cell_type":"code","source":"alignment_model = None\nmetadata = None\nalignment_available = False\n\nif CONFIG['use_alignment']:\n    try:\n        alignment_model, metadata = whisperx.load_align_model(\n            language_code=CONFIG['language'],\n            device=device\n        )\n        alignment_available = True\n        print(f\"Bengali Alignment Model loaded\")\n    except Exception as e:\n        print(f\"Bengali alignment model unavailable: {e}\")\n        print(\"Proceeding with segment-level timestamps only\")\n        alignment_available = False\n        CONFIG['use_alignment'] = False\nelse:\n    print(\"üìù Alignment disabled in configuration\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Audio Processing Pipeline**","metadata":{}},{"cell_type":"code","source":"def transcribe_bengali_audio(audio_path, use_alignment=True):\n    try:\n        # Step 1: Load audio\n        print(f\"Loading: {os.path.basename(audio_path)}\")\n        audio = whisperx.load_audio(audio_path)\n        \n        audio_duration = len(audio) / 16000\n\n        #add preprocessing here\n        \n        if CONFIG['max_audio_length'] is not None and audio_duration > CONFIG['max_audio_length']:\n            print(f\"Audio too long ({audio_duration:.1f}s), truncating to {CONFIG['max_audio_length']}s\")\n            audio = audio[:CONFIG['max_audio_length'] * 16000]\n        else:\n            print(f\"üìä Audio duration: {audio_duration:.1f}s\")\n        \n        # Step 2: Transcribe with custom Bengali Whisper model\n\n        result = whisper_model.transcribe(\n            audio,\n            batch_size=CONFIG['batch_size'],\n            language=CONFIG['language'],\n            chunk_size=CONFIG['chunk_size'],\n            print_progress=False\n        )\n        \n        # Step 3: Apply WhisperX's built-in alignment (uses your wav2vec2 from alignment.py)\n        \n        if use_alignment and alignment_available and alignment_model is not None:\n            try:\n                result = whisperx.align(\n                    result[\"segments\"],\n                    alignment_model,\n                    metadata,\n                    audio,\n                    device,\n                    return_char_alignments=False,\n                    print_progress=False\n                )\n            except Exception as align_error:\n                print(f\"Alignment failed: {align_error}\")\n        \n        # Step 4: Extract transcription text\n        full_transcription = \"\"\n        \n        if \"segments\" in result and result[\"segments\"]:\n            segment_texts = []\n            for segment in result[\"segments\"]:\n                if \"text\" in segment and segment[\"text\"].strip():\n                    segment_texts.append(segment[\"text\"].strip())\n            full_transcription = \" \".join(segment_texts)\n        \n        # Step 5: Clean up transcription\n        if full_transcription:\n            full_transcription = \" \".join(full_transcription.split()).strip()\n        \n        print(f\"Transcription complete ({len(full_transcription)} chars)\")\n        return full_transcription\n        \n    except Exception as e:\n        print(f\"Error processing {audio_path}: {e}\")\n        return \"\"\n\nprint(\"Audio transcription pipeline ready!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Check Test Files**","metadata":{}},{"cell_type":"code","source":"test_files = []\nif os.path.exists(TEST_AUDIO_PATH):\n    # Find all .wav files in test directory\n    wav_files = glob.glob(os.path.join(TEST_AUDIO_PATH, \"*.wav\"))\n    test_files = [os.path.basename(f) for f in wav_files]\n    test_files.sort()  # Sort for consistent processing order\n    \n    print(f\"Found {len(test_files)} test audio files\")\n    \n    # Show first few files as preview\n    print(f\"\\nFirst 5 test files:\")\n    for i, fname in enumerate(test_files[:5]):\n        fpath = os.path.join(TEST_AUDIO_PATH, fname)\n        if os.path.exists(fpath):\n            # Get file size\n            size_mb = os.path.getsize(fpath) / (1024*1024)\n            print(f\"  {i+1:2d}. {fname} ({size_mb:.1f} MB)\")\n        else:\n            print(f\"  {i+1:2d}. {fname} (file not found)\")\n    \n    if len(test_files) > 5:\n        print(f\"     ... and {len(test_files) - 5} more files\")\n        \nelse:\n    print(f\"Test audio directory not found: {TEST_AUDIO_PATH}\")\n    print(\"Please update TEST_AUDIO_PATH in the configuration section\")\n    \n\nif len(test_files) == 0:\n    print(\"No test files found! Please check your path configuration.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Inference Loop**","metadata":{}},{"cell_type":"code","source":"\nif len(test_files) == 0:\n    print(\"No test files found. Please check your paths and rerun.\")\nelse:\n    results = []\n    processed_count = 0\n    error_count = 0\n    \n    for idx, test_filename in enumerate(tqdm(test_files, desc=\"Processing audio files\")):\n        try:\n            audio_file_path = os.path.join(TEST_AUDIO_PATH, test_filename)\n            \n            if not os.path.exists(audio_file_path):\n                print(f\"File not found: {audio_file_path}\")\n                results.append({\n                    \"filename\": test_filename,\n                    \"transcription\": \"\"\n                })\n                error_count += 1\n                continue\n            \n            # Transcribe the audio file\n            transcription = transcribe_bengali_audio(\n                audio_file_path, \n                use_alignment=CONFIG['use_alignment']\n            )\n            \n            # Store result\n            results.append({\n                \"filename\": test_filename,\n                \"transcription\": transcription\n            })\n            \n            processed_count += 1\n            \n            # Show progress and sample result\n            if idx == 0:  # Show first result as example\n                print(f\"\\nFirst Sample:\")\n                print(f\"   Text: {transcription[:100]}{'...' if len(transcription) > 100 else ''}\")\n            \n            # Memory cleanup every 10 files\n            if (idx + 1) % 10 == 0:\n                print(f\"Memory cleanup... ({idx + 1}/{len(test_files)} processed)\")\n                gc.collect()\n                if device == \"cuda\":\n                    torch.cuda.empty_cache()\n            \n        except KeyboardInterrupt:\n            print(\"\\nProcessing interrupted by user\")\n            break\n            \n        except Exception as e:\n            print(f\"Error processing {test_filename}: {e}\")\n            results.append({\n                \"filename\": test_filename,\n                \"transcription\": \"\"\n            })\n            error_count += 1\n            continue\n    \n    # Final processing summary\n    print(f\"Successfully processed: {processed_count}/{len(test_files)}\")\n    print(f\"Errors encountered: {error_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Submission CSV**","metadata":{}},{"cell_type":"code","source":"if len(test_files) > 0 and 'results' in locals():\n    print(\"üìù Creating competition submission file...\")\n    \n    submission_df = pd.DataFrame(results)\n    \n    # Ensure correct column names for competition\n    submission_df.columns = ['filename', 'transcription']\n    \n    # Show submission preview\n    print(\"Submission Preview:\")\n    print(submission_df.head(10))\n    \n    submission_df.to_csv(OUTPUT_CSV_PATH, index=False, encoding='utf-8')\n    \n    print(f\"\\nSubmission saved to: {OUTPUT_CSV_PATH}\")\n        \nelse:\n    print(\"Error.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}